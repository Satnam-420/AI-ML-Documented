{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ce3dd86",
   "metadata": {},
   "source": [
    "# Course Notes: AI/ML, DSA, and More\n",
    "## This notebook documents my learnings from various courses as I work toward becoming a top-tier AI/ML and Data Science engineer. My goal is to complete certifications in AI/ML, cloud, and DevOps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ae0aa1",
   "metadata": {},
   "source": [
    "## Week 1 Learnings\n",
    "## Day 3 (May 3 , 2025): Part 1 - Intro to Machine Learning (Andrew Ngâ€™s ML Specialization)\n",
    "### - What is ML: Making predictions or decisions using data.\n",
    "### - Supervised Learning: Uses labeled data for training.\n",
    " ### - Regression: Predict continuous values (e.g., house prices).\n",
    " ### - Classification: Predict categories (e.g., spam vs. not spam).\n",
    "### - Unsupervised Learning: Finds patterns in unlabeled data (e.g., clustering customers).\n",
    "\n",
    "## Day 4 (May 4 , 2025): Regression Models (Week 1)\n",
    "### - Linear Regression: Fits a line to data to predict continuous values (e.g., house prices).\n",
    "### - Cost Function: Measures the error between predicted and actual values, used to optimize the model.\n",
    "\n",
    "## Day 5 (May 5 , 2025): Regression Models (Week 1)\n",
    "### > Learned about cost function: mean squared error\n",
    "### > Explored its parabolic visualization\n",
    "### > Studied parameter optimization\n",
    "### > Analyzed regression errors\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401f39c1",
   "metadata": {},
   "source": [
    "## Day 6: Gradient Descent (Week 1)\n",
    "## -Gradient Descent: Iteratively adjusts parameters to minimize the cost function by taking steps in the direction of the steepest descent.\n",
    "## - Implementation: Involves choosing a learning rate (step size) and iterating until convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2a9b83",
   "metadata": {},
   "source": [
    "## Day 7: Multiple Linear Regression (Week 1)\n",
    "### Multiple Linear Regression: Extends linear regression to predict outcomes using multiple features (e.g., y = w0 + w1x1 + w2x2 + ... + wnxn).\n",
    "### Cost Function: Measures error across all features, minimized by adjusting weights (w0, w1, ..., wn).\n",
    "\n",
    "## Days 8-10: Review of Fundamentals (Week 1)\n",
    "### Gradient Descent Review: Revisited parameter optimization steps, focusing on how gradients guide updates (from Day 6).\n",
    "### Multiple Linear Regression Review: Recapped cost function and multi-feature models (from Day 7).\n",
    "### Calculus Review: Reinforced derivatives and their role in gradient descent (from Day 8)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15012e1",
   "metadata": {},
   "source": [
    "## Day 11: Gradient Descent for Multiple Linear Regression (Week 2)\n",
    "### Gradient Descent for Multiple Linear Regression: Applied gradient descent to optimize parameters (weights) across multiple features by minimizing the cost function.\n",
    "### Learning Rate Tuning: Adjusted the learning rate to control step size, ensuring convergence without overshooting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3440b623",
   "metadata": {},
   "source": [
    "# Day 12: Logistic Regression Basics (Week 2)\n",
    "## Logistic Regression: Introduced for classification; uses the sigmoid function to predict probabilities (e.g., output between 0 and 1).\n",
    "## Cost Function: Defined the log-loss (binary cross-entropy) to measure error in classification.\n",
    "\n",
    "# Day 13: Logistic Regression Optimization (Week 2)\n",
    "## Gradient Descent for Logistic Regression: Applied gradient descent to minimize the log-loss cost function by updating weights.\n",
    "## Sigmoid Function: Explored how the sigmoid maps inputs to probabilities (e.g., z = w0 + w1x1 + w2x2, sigmoid(z) = 1/(1 + e^(-z))).\n",
    "\n",
    "#   Day 14 Multi-Class Classification (Week 2)\n",
    "## Multi-Class Classification: Extended logistic regression using softmax regression for multiple classes (e.g., predicting more than two outcomes).\n",
    "## Application: Useful for problems like classifying Iris species (future Kaggle project reference).\n",
    "\n",
    "# Day 15: Review and Offline Practice (Week 2)\n",
    "## Review: Revisited logistic regression concepts (sigmoid, cost function, gradient descent) to solidify understanding.\n",
    "## Offline Practice: Wrote code snippets by hand to implement logistic regression, focusing on the gradient descent update rule.\n",
    "\n",
    "# Day 16: Regularization in Logistic Regression (Week 3)\n",
    "\n",
    "## Regularization: Learned L2 regularization to prevent overfitting by adding a penalty term (lambda * sum of squared weights) to the cost function.\n",
    "## Impact: Helps generalize the model by reducing the magnitude of weights, improving performance on unseen data.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
