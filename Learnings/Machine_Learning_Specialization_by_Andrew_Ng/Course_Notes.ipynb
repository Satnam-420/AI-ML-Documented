{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ce3dd86",
   "metadata": {},
   "source": [
    "# Course Notes: AI/ML, DSA, and More\n",
    "## This notebook documents my learnings from various courses as I work toward becoming a top-tier AI/ML and Data Science engineer. My goal is to complete certifications in AI/ML, cloud, and DevOps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ae0aa1",
   "metadata": {},
   "source": [
    "## Week 1 Learnings\n",
    "## Day 3 (May 3 , 2025): Part 1 - Intro to Machine Learning (Andrew Ngâ€™s ML Specialization)\n",
    "### - What is ML: Making predictions or decisions using data.\n",
    "### - Supervised Learning: Uses labeled data for training.\n",
    " ### - Regression: Predict continuous values (e.g., house prices).\n",
    " ### - Classification: Predict categories (e.g., spam vs. not spam).\n",
    "### - Unsupervised Learning: Finds patterns in unlabeled data (e.g., clustering customers).\n",
    "\n",
    "## Day 4 (May 4 , 2025): Regression Models (Week 1)\n",
    "### - Linear Regression: Fits a line to data to predict continuous values (e.g., house prices).\n",
    "### - Cost Function: Measures the error between predicted and actual values, used to optimize the model.\n",
    "\n",
    "## Day 5 (May 5 , 2025): Regression Models (Week 1)\n",
    "### > Learned about cost function: mean squared error\n",
    "### > Explored its parabolic visualization\n",
    "### > Studied parameter optimization\n",
    "### > Analyzed regression errors\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401f39c1",
   "metadata": {},
   "source": [
    "## Day 6: Gradient Descent (Week 1)\n",
    "## -Gradient Descent: Iteratively adjusts parameters to minimize the cost function by taking steps in the direction of the steepest descent.\n",
    "## - Implementation: Involves choosing a learning rate (step size) and iterating until convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2a9b83",
   "metadata": {},
   "source": [
    "## Day 7: Multiple Linear Regression (Week 1)\n",
    "### Multiple Linear Regression: Extends linear regression to predict outcomes using multiple features (e.g., y = w0 + w1x1 + w2x2 + ... + wnxn).\n",
    "### Cost Function: Measures error across all features, minimized by adjusting weights (w0, w1, ..., wn).\n",
    "\n",
    "## Days 8-10: Review of Fundamentals (Week 1)\n",
    "### Gradient Descent Review: Revisited parameter optimization steps, focusing on how gradients guide updates (from Day 6).\n",
    "### Multiple Linear Regression Review: Recapped cost function and multi-feature models (from Day 7).\n",
    "### Calculus Review: Reinforced derivatives and their role in gradient descent (from Day 8)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15012e1",
   "metadata": {},
   "source": [
    "## Day 11: Gradient Descent for Multiple Linear Regression (Week 2)\n",
    "### Gradient Descent for Multiple Linear Regression: Applied gradient descent to optimize parameters (weights) across multiple features by minimizing the cost function.\n",
    "### Learning Rate Tuning: Adjusted the learning rate to control step size, ensuring convergence without overshooting."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
